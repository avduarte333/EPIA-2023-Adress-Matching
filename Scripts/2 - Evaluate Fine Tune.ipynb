{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample, losses, util, SentenceTransformer, CrossEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Only run the following 2 cells, the first time the program is executed.</h3>\n",
    "We are creating upfront the auxiliar databases to increase predicting speed. After doing this, we can skip this step and just load them for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/datasets/'\n",
    "ALL_DATA= pd.read_csv(data_folder + \"database.csv\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "indexes_CP_1 = ALL_DATA['First Digit CP4'].loc[lambda x: x==1].index\n",
    "indexes_CP_2 = ALL_DATA['First Digit CP4'].loc[lambda x: x==2].index\n",
    "indexes_CP_3 = ALL_DATA['First Digit CP4'].loc[lambda x: x==3].index\n",
    "indexes_CP_4 = ALL_DATA['First Digit CP4'].loc[lambda x: x==4].index\n",
    "indexes_CP_5 = ALL_DATA['First Digit CP4'].loc[lambda x: x==5].index\n",
    "indexes_CP_6 = ALL_DATA['First Digit CP4'].loc[lambda x: x==6].index\n",
    "indexes_CP_7 = ALL_DATA['First Digit CP4'].loc[lambda x: x==7].index\n",
    "indexes_CP_8 = ALL_DATA['First Digit CP4'].loc[lambda x: x==8].index\n",
    "indexes_CP_9 = ALL_DATA['First Digit CP4'].loc[lambda x: x==9].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('/home/bi_encoder tuned')\n",
    "DATABASE_Embeddings = ALL_DATA['Morada_Full_Normalizada'].to_list()\n",
    "DATABASE_Embeddings = model.encode(DATABASE_Embeddings) # Isto retorna ndArray\n",
    "\n",
    "#Write CSV's of DB and all the Filtered by CP4\n",
    "DATABASE_Embeddings_pd = pd.DataFrame(DATABASE_Embeddings); DATABASE_Embeddings_pd.to_csv(data_folder + 'DB_Embeddings_Full.csv', index=False)\n",
    "DB_CP1_ = DATABASE_Embeddings_pd.iloc[indexes_CP_1,:]; DB_CP1_.to_csv(data_folder + 'DB_Embeddings_1.csv', index=False)\n",
    "DB_CP2_ = DATABASE_Embeddings_pd.iloc[indexes_CP_2,:]; DB_CP2_.to_csv(data_folder + 'DB_Embeddings_2.csv', index=False)\n",
    "DB_CP3_ = DATABASE_Embeddings_pd.iloc[indexes_CP_3,:]; DB_CP3_.to_csv(data_folder + 'DB_Embeddings_3.csv', index=False)\n",
    "DB_CP4_ = DATABASE_Embeddings_pd.iloc[indexes_CP_4,:]; DB_CP4_.to_csv(data_folder + 'DB_Embeddings_4.csv', index=False)\n",
    "DB_CP5_ = DATABASE_Embeddings_pd.iloc[indexes_CP_5,:]; DB_CP5_.to_csv(data_folder + 'DB_Embeddings_5.csv', index=False)\n",
    "DB_CP6_ = DATABASE_Embeddings_pd.iloc[indexes_CP_6,:]; DB_CP6_.to_csv(data_folder + 'DB_Embeddings_6.csv', index=False)\n",
    "DB_CP7_ = DATABASE_Embeddings_pd.iloc[indexes_CP_7,:]; DB_CP7_.to_csv(data_folder + 'DB_Embeddings_7.csv', index=False)\n",
    "DB_CP8_ = DATABASE_Embeddings_pd.iloc[indexes_CP_8,:]; DB_CP8_.to_csv(data_folder + 'DB_Embeddings_8.csv', index=False)\n",
    "DB_CP9_ = DATABASE_Embeddings_pd.iloc[indexes_CP_9,:]; DB_CP9_.to_csv(data_folder + 'DB_Embeddings_9.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>After Pre-Embedding Step</h3>\n",
    "Assuming auxiliar databases are created, let's load them, load the test dataset and start inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/datasets/'\n",
    "\n",
    "test_data = pd.read_csv(data_folder + \"test_data.csv\")\n",
    "ALL_DATA= pd.read_csv(data_folder + \"database.csv\", encoding=\"utf-8\")\n",
    "DATABASE_Embeddings= pd.read_csv(data_folder + \"DB_embeddings_Full.csv\")\n",
    "\n",
    "DB_CP1_ = pd.read_csv(data_folder + \"DB_Embeddings_1.csv\")\n",
    "DB_CP2_ = pd.read_csv(data_folder + \"DB_Embeddings_2.csv\")\n",
    "DB_CP3_ = pd.read_csv(data_folder + \"DB_Embeddings_3.csv\")\n",
    "DB_CP4_ = pd.read_csv(data_folder + \"DB_Embeddings_4.csv\")\n",
    "DB_CP5_ = pd.read_csv(data_folder + \"DB_Embeddings_5.csv\")\n",
    "DB_CP6_ = pd.read_csv(data_folder + \"DB_Embeddings_6.csv\")\n",
    "DB_CP7_ = pd.read_csv(data_folder + \"DB_Embeddings_7.csv\")\n",
    "DB_CP8_ = pd.read_csv(data_folder + \"DB_Embeddings_8.csv\")\n",
    "DB_CP9_ = pd.read_csv(data_folder + \"DB_Embeddings_9.csv\")\n",
    "\n",
    "\n",
    "#Convert Embeddings from Pandas DF to numpy and float\n",
    "DATABASE_Embeddings = DATABASE_Embeddings.to_numpy().astype(float)\n",
    "DB_CP1_ = DB_CP1_.to_numpy().astype(float)\n",
    "DB_CP2_ = DB_CP2_.to_numpy().astype(float)\n",
    "DB_CP3_ = DB_CP3_.to_numpy().astype(float)\n",
    "DB_CP4_ = DB_CP4_.to_numpy().astype(float)\n",
    "DB_CP5_ = DB_CP5_.to_numpy().astype(float)\n",
    "DB_CP6_ = DB_CP6_.to_numpy().astype(float)\n",
    "DB_CP7_ = DB_CP7_.to_numpy().astype(float)\n",
    "DB_CP8_ = DB_CP8_.to_numpy().astype(float)\n",
    "DB_CP9_ = DB_CP9_.to_numpy().astype(float)\n",
    "\n",
    "\n",
    "\n",
    "#Load Indexes\n",
    "indexes_CP_1 = ALL_DATA['First Digit CP4'].loc[lambda x: x==1].index\n",
    "indexes_CP_2 = ALL_DATA['First Digit CP4'].loc[lambda x: x==2].index\n",
    "indexes_CP_3 = ALL_DATA['First Digit CP4'].loc[lambda x: x==3].index\n",
    "indexes_CP_4 = ALL_DATA['First Digit CP4'].loc[lambda x: x==4].index\n",
    "indexes_CP_5 = ALL_DATA['First Digit CP4'].loc[lambda x: x==5].index\n",
    "indexes_CP_6 = ALL_DATA['First Digit CP4'].loc[lambda x: x==6].index\n",
    "indexes_CP_7 = ALL_DATA['First Digit CP4'].loc[lambda x: x==7].index\n",
    "indexes_CP_8 = ALL_DATA['First Digit CP4'].loc[lambda x: x==8].index\n",
    "indexes_CP_9 = ALL_DATA['First Digit CP4'].loc[lambda x: x==9].index\n",
    "indexes_DB = ALL_DATA['First Digit CP4'].loc[lambda x:   x!=-2].index #This may look strange, but it is to keep the code more simple and concise on the predicting loop\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load bi-encoder and cross-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cross = CrossEncoder('/home/cross_encoder tuned')\n",
    "model = SentenceTransformer('/home/bi_encoder tuned')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary for making predicting loop more clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_Dicts = {\n",
    "  \"1\": DB_CP1_,\n",
    "  \"2\": DB_CP2_,\n",
    "  \"3\": DB_CP3_,\n",
    "  \"4\": DB_CP4_,\n",
    "  \"5\": DB_CP5_,\n",
    "  \"6\": DB_CP6_,\n",
    "  \"7\": DB_CP7_,\n",
    "  \"8\": DB_CP8_,\n",
    "  \"9\": DB_CP9_,\n",
    "  \"-1\":DATABASE_Embeddings,\n",
    "}\n",
    "\n",
    "\n",
    "DB_Indexes ={\n",
    "  \"1\": indexes_CP_1,\n",
    "  \"2\": indexes_CP_2,\n",
    "  \"3\": indexes_CP_3,\n",
    "  \"4\": indexes_CP_4,\n",
    "  \"5\": indexes_CP_5,\n",
    "  \"6\": indexes_CP_6,\n",
    "  \"7\": indexes_CP_7,\n",
    "  \"8\": indexes_CP_8,\n",
    "  \"9\": indexes_CP_9,\n",
    "  \"-1\":indexes_DB,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando TOP 10 Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discovered_addresses = []\n",
    "discovered_arteries  = []\n",
    "discovered_doors = []\n",
    "match_confidence = []\n",
    "label = []\n",
    "scores = []\n",
    "\n",
    "number_of_records_retrieved = 10\n",
    "\n",
    "for i in tqdm(range(len(test_data))):\n",
    "\n",
    "        firstCP4_digit = test_data.at[i, \"First Digit CP4\"]\n",
    "\n",
    "        \n",
    "        encode_real = model.encode(test_data.at[i, 'Morada_Full']).astype(float)\n",
    "        cos_sim = util.cos_sim(encode_real, DB_Dicts[str(firstCP4_digit)])\n",
    "        values, indices = torch.topk(cos_sim,number_of_records_retrieved) # Return top-10 most probable matches\n",
    "\n",
    "        for j in range(number_of_records_retrieved): #Iterate over the top-10 returned matches\n",
    "                #Calculate cross_encoder score between unnormalized_address and returned_address_i\n",
    "                scores.append(model_cross.predict([ALL_DATA.at[DB_Indexes[str(firstCP4_digit)][indices[0][j].item()], 'Morada_Full_Normalizada'], test_data.at[i, 'Morada_Full']]))\n",
    "\n",
    "\n",
    "        #Append to auxiliar lists the records that will be written in the results file. We write the values corresponding to the highest scored pair in the top-10 retrieved.\n",
    "        discovered_addresses.append(ALL_DATA.at[DB_Indexes[str(firstCP4_digit)][indices[0][np.argmax(scores)].item()], 'Morada_Full_Normalizada'])\n",
    "        discovered_arteries.append(ALL_DATA.at[DB_Indexes[str(firstCP4_digit)][indices[0][np.argmax(scores)].item()], 'ID_ARTERIA'])\n",
    "        discovered_doors.append(ALL_DATA.at[DB_Indexes[str(firstCP4_digit)][indices[0][np.argmax(scores)].item()], 'ID_PORTA'])\n",
    "        match_confidence.append(np.max(scores))\n",
    "        scores = []\n",
    "\n",
    "\n",
    "\n",
    "test_data['Discovered Address'] = discovered_addresses\n",
    "test_data['Discovered Artery'] = discovered_arteries\n",
    "test_data['Discovered Door'] = discovered_doors\n",
    "test_data['Match Confidence'] = match_confidence\n",
    "\n",
    "test_data.to_csv('results.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aac6930534f871d314aca2610c2357ec063ba4065ca1d2a97333736987f270c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
